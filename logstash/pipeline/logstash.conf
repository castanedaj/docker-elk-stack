input {
  tcp {
    port => 5000
    type => syslog
    codec => multiline {
      pattern => "%{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}"
      what => "previous"
      multiline_tag => "drop"
    }
  }
  # udp {
  #   port => 5000
  #   type => syslog
  #   codec => multiline {
  #     pattern => "%{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}"
  #     what => "previous"
  #     multiline_tag => "drop"
  #   }
  # }
  #stdin { }
  beats {
    port => 5043
    # codec => multiline {
    #   pattern => "%{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}"
    #   what => "previous"
    # }
  }
}

filter {
  if "drop" in [tags] {
    drop { }
  }

  # ===== this should parse syslog =====
  if [type] == "syslog" {
    grok {
      match => { "message" => "(?m)%{SYSLOG5424PRI}%{NONNEGINT:ver} +(?:%{TIMESTAMP_ISO8601:ts}|-) +(?:%{HOSTNAME:containerid}|-) +(?:%{NOTSPACE:containername}|-) +(?:%{NOTSPACE:proc}|-) +(?:%{WORD:msgid}|-) +(?:%{SYSLOG5424SD:sd}|-|) +%{GREEDYDATA:msg}" }
      add_tag => [ "syslog" ]
    }
    syslog_pri { }
    date {
      match => [ "ts", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "MMM dd yyyy HH:mm:ss", "MMM  d yyyy HH:mm:ss", "ISO8601" ]
      remove_field => [ "ts" ]
    }
  }

  # we move the message field into msg field for future analysis
  if ![msg] {
    mutate {
      add_field => { "msg" => "%{message}" }
    }
  }

  # ==== we try to parse apache log ====
  grok {
    #match => { "msg" => "%{IP:client_host}%{SPACE}%{GREEDYDATA:msg}" }
    match => { "msg" => "%{COMMONAPACHELOG}" }
    add_tag => [ "httpd_log" ]
    add_field => { "apache_timestamp" => "%{timestamp}" }
    remove_field => [ "timestamp" ]
  }

  # ==== if the apache log parse was fine we should have apache_timestamp field ready ====
  date {
    match => [ "apache_timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    remove_field => [ "apache_timestamp" ]
  }

  # ==== since we have the clientip we extract geo data aswell ====
  geoip {
      source => "clientip"
      add_tag => [ "geoip" ]
  }
  
  # ==== if nor syslog nor apache log was parse we try with pmis log ====
  mutate {
    remove_tag => [ "_grokparsefailure" ]
  }

  if "httpd_log" not in [tags] {
    grok {
      # we try to match the pmis log pattern
      match => { "msg" => "(?:%{TIMESTAMP_ISO8601:event_ts}) (?:\[%{GREEDYDATA:os.name}\]) ?%{GREEDYDATA:msg}" }
      overwrite => [ "msg" ]
    }
    # dissect {
    #   mapping => { "msg" => "%{ts} %{+ts} [%{os.name}] %{class} %{custom_data}" }
    # }
    # date {
    #   match => [ "ts", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
    #   remove_field => [ "ts" ]
    # }
    if "_grokparsefailure" not in [tags] {
      mutate {
        add_tag => "pmis_log"
      }

      dissect {
        mapping => { "msg" => "%{event_class} %{custom_data}" }
      }
      if [custom_data] {
        # we try to convert the custom_data as json data (only if not empty)
        json {
          source => "custom_data"
        }
        if "_jsonparsefailure" not in [tags] {
          # if the json parse passed we replace the msg field with custom_data
          mutate {
            replace => { "msg" => "%{custom_data}" }
          }
        }
        if "_jsonparsefailure" in [tags] {
          # we remove the event_class field if json parse fail
          mutate {
            remove_field => [ "event_class" ]
          }
        }
      }
    }
  }
  
  # ==== we do some cleanup ====
  mutate {
    # this should be the last mutate
    replace => [ "message", "%{msg}" ]
    remove_field => [ "msg", "custom_data" ]
    # remove_tag => [ 
    #   "_grokparsefailure", 
    #   "_dissectfailure", 
    #   "_geoip_lookup_failure",
    #   "_dateparsefailure",
    #   "_jsonparsefailure"
    # ]
  }
}

output {
    elasticsearch { 
      hosts => ["${ES_HOST}"]
      user => "${ES_USER}"
      password => "${ES_PASSWORD}"
      #ssl => true
      ssl_certificate_verification => false
      timeout => 240
      #cacert => '${LS_HOME}/config/ssl/ca.pem'
    }
    stdout { codec => rubydebug }
}